{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9895be63",
   "metadata": {},
   "source": [
    "### Gemini 2.0 flash lite VS Tuned Gemini 2.0 flash lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "766e695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project modules (config, utils, vllm_handler) imported successfully.\n",
      "\n",
      "CRITICAL WARNING: vllm_handler.analyze_content is missing the 'model_id_override' parameter!\n",
      "Please update src/vllm_handler.py and RESTART THE KERNEL before proceeding.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Imports and Setup ---\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown # For displaying DataFrames and Markdown\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.getLogger(\"google.api_core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"google.auth\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"PIL\").setLevel(logging.WARNING) # Suppress PIL logs if noisy\n",
    "\n",
    "# Add src directory to Python path to import project modules\n",
    "# Assumes notebook is in the project root directory (e.g., GDG_HACKATHON/)\n",
    "project_root = os.path.abspath('.') # Use '.' if notebook is in root\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"Added {src_path} to sys.path\")\n",
    "\n",
    "# Import project modules\n",
    "try:\n",
    "    import config\n",
    "    import utils\n",
    "    import vllm_handler\n",
    "    # Import specific function to check signature easily\n",
    "    from vllm_handler import analyze_content\n",
    "    import inspect # To inspect function signature\n",
    "\n",
    "    print(\"Project modules (config, utils, vllm_handler) imported successfully.\")\n",
    "\n",
    "    # Verify the handler has the modified function signature\n",
    "    sig = inspect.signature(analyze_content)\n",
    "    if 'model_id_override' not in sig.parameters:\n",
    "         print(\"\\nCRITICAL WARNING: vllm_handler.analyze_content is missing the 'model_id_override' parameter!\")\n",
    "         print(\"Please update src/vllm_handler.py and RESTART THE KERNEL before proceeding.\")\n",
    "         # Optionally raise an error to stop execution\n",
    "         # raise AttributeError(\"analyze_content function signature is incorrect.\")\n",
    "    else:\n",
    "         print(\"Verified: vllm_handler.analyze_content has 'model_id_override' parameter.\")\n",
    "\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(\"1. This notebook is saved in the project's root directory (GDG_HACKATHON/).\")\n",
    "    print(\"2. The src directory and its files (config.py, utils.py, vllm_handler.py) exist.\")\n",
    "    # Stop execution if modules can't be loaded\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfb83460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model ID: gemini-2.0-flash-lite-001\n",
      "Tuned Model ID: projects/248124319532/locations/europe-west4/models/8219698240602243072\n",
      "Output Directory: /home/harishi/common_drive/Downloads/projects/GDG_HACKATHON/my_learning/outputs\n",
      "Input Directory: /home/harishi/common_drive/Downloads/projects/GDG_HACKATHON/inputs\n",
      "Project ID: maximal-cider-458611-r5\n",
      "Region: europe-west4\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Configuration ---\n",
    "\n",
    "BASE_MODEL_ID = \"gemini-2.0-flash-lite-001\"\n",
    "# IMPORTANT: Verify this is your correct tuned model ID and region\n",
    "TUNED_MODEL_ID = \"projects/248124319532/locations/europe-west4/models/8219698240602243072\"\n",
    "\n",
    "# Define output file paths (relative to the project root)\n",
    "output_dir = os.path.join(project_root, 'outputs')\n",
    "COMPARISON_OUTPUT_FILENAME = \"comparison_results.json\"\n",
    "GRAPH_OUTPUT_FILENAME = \"comparison_graph.png\"\n",
    "COMPARISON_TABLE_FILENAME = \"comparison_table.csv\" # For saving the DataFrame\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Base Model ID: {BASE_MODEL_ID}\")\n",
    "print(f\"Tuned Model ID: {TUNED_MODEL_ID}\")\n",
    "print(f\"Output Directory: {output_dir}\")\n",
    "print(f\"Input Directory: {config.INPUT_DIR}\") # From imported config\n",
    "print(f\"Project ID: {config.GCP_PROJECT_ID}\")\n",
    "print(f\"Region: {config.GCP_REGION}\") # Make sure this matches the model region (europe-west4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3227e9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK initialized successfully for comparison.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Initialize Vertex AI ---\n",
    "\n",
    "# Initialize Vertex AI SDK (important before making API calls)\n",
    "# vllm_handler should handle the actual vertexai.init call via initialize_vertex_ai()\n",
    "# We call it once here to ensure it's done before the loop.\n",
    "if vllm_handler.initialize_vertex_ai():\n",
    "    print(\"Vertex AI SDK initialized successfully for comparison.\")\n",
    "else:\n",
    "    print(\"ERROR: Vertex AI SDK initialization failed. Cannot proceed.\")\n",
    "    # Optionally raise an error to stop the notebook\n",
    "    # raise RuntimeError(\"Vertex AI SDK failed to initialize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac13b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 05:40:22,826 - INFO - Recursively scanning for supported files in: /home/harishi/common_drive/Downloads/projects/GDG_HACKATHON/inputs\n",
      "2025-05-03 05:40:22,828 - INFO - Found 52 supported files across all subdirectories.\n",
      "2025-05-03 05:40:22,829 - INFO - Found 52 files to process.\n",
      "2025-05-03 05:40:22,829 - INFO - \n",
      "--- Running Base Model: gemini-2.0-flash-lite-001 ---\n",
      "2025-05-03 05:40:22,830 - INFO - Processing ../inputs/jpeg/1.jpeg with BASE model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'set_model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m file_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Call analyze_content, overriding the model ID\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Use the imported function directly for clarity\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Assuming analyze_content uses a global or default model ID, set it before calling the function\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mset_model_id\u001b[49m(BASE_MODEL_ID)  \u001b[38;5;66;03m# Replace with the actual function or method to set the model ID\u001b[39;00m\n\u001b[1;32m     34\u001b[0m result_str_base \u001b[38;5;241m=\u001b[39m analyze_content(file_path)\n\u001b[1;32m     35\u001b[0m file_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_model_id' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Run Comparison Analysis ---\n",
    "\n",
    "comparison_results_list = [] # Store results as a list of dicts for easier DataFrame creation\n",
    "base_model_times = []\n",
    "tuned_model_times = []\n",
    "files_processed_base = 0\n",
    "files_processed_tuned = 0\n",
    "files_error_base = 0\n",
    "files_error_tuned = 0\n",
    "\n",
    "# Get Input Files\n",
    "# Use absolute path for input directory for robustness\n",
    "abs_input_dir = os.path.abspath(config.INPUT_DIR)\n",
    "input_files = utils.get_input_files(abs_input_dir)\n",
    "\n",
    "\n",
    "if not input_files:\n",
    "    logging.warning(f\"No input files found in {abs_input_dir}. Exiting comparison.\")\n",
    "else:\n",
    "    logging.info(f\"Found {len(input_files)} files to process.\")\n",
    "\n",
    "    # --- Run Base Model ---\n",
    "    logging.info(f\"\\n--- Running Base Model: {BASE_MODEL_ID} ---\")\n",
    "    total_start_time_base = time.time()\n",
    "    for file_path in input_files: # file_path is now absolute\n",
    "        relative_path = os.path.relpath(file_path, project_root) # Get path relative to project root\n",
    "        logging.info(f\"Processing {relative_path} with BASE model...\")\n",
    "        file_start_time = time.time()\n",
    "\n",
    "        # Call analyze_content, overriding the model ID\n",
    "        # Use the imported function directly for clarity\n",
    "        result_str_base = analyze_content(file_path, model_id_override=BASE_MODEL_ID)\n",
    "        file_end_time = time.time()\n",
    "        duration = file_end_time - file_start_time\n",
    "        files_processed_base += 1\n",
    "\n",
    "        result_entry = {\n",
    "            \"file\": relative_path,\n",
    "            \"base_model_output\": result_str_base,\n",
    "            \"base_model_time_sec\": None,\n",
    "            \"tuned_model_output\": \"N/A\", # Placeholder\n",
    "            \"tuned_model_time_sec\": None, # Placeholder\n",
    "        }\n",
    "\n",
    "        if result_str_base.startswith(\"Error:\"):\n",
    "            logging.error(f\"Base model error for {relative_path}: {result_str_base}\")\n",
    "            files_error_base += 1\n",
    "        else:\n",
    "            base_model_times.append(duration)\n",
    "            result_entry[\"base_model_time_sec\"] = duration\n",
    "            logging.info(f\"Base model success for {relative_path} in {duration:.2f}s\")\n",
    "\n",
    "        comparison_results_list.append(result_entry) # Add entry even if error occurred\n",
    "\n",
    "    total_end_time_base = time.time()\n",
    "    logging.info(f\"Base model run finished. Total time: {total_end_time_base - total_start_time_base:.2f}s\")\n",
    "    logging.info(f\"Base model: {files_processed_base - files_error_base} successful, {files_error_base} errors.\")\n",
    "\n",
    "    # --- Run Tuned Model ---\n",
    "    logging.info(f\"\\n--- Running Tuned Model: {TUNED_MODEL_ID} ---\")\n",
    "    total_start_time_tuned = time.time()\n",
    "\n",
    "    # Find the corresponding entry in the list to update\n",
    "    for i, file_path in enumerate(input_files): # file_path is absolute\n",
    "        relative_path = os.path.relpath(file_path, project_root) # Get path relative to project root\n",
    "        logging.info(f\"Processing {relative_path} with TUNED model...\")\n",
    "        file_start_time = time.time()\n",
    "\n",
    "        # Call analyze_content using the tuned model ID (explicitly)\n",
    "        # Ensure the 'AttributeError' is fixed before this step!\n",
    "        result_str_tuned = analyze_content(file_path, model_id_override=TUNED_MODEL_ID)\n",
    "        file_end_time = time.time()\n",
    "        duration = file_end_time - file_start_time\n",
    "        files_processed_tuned += 1\n",
    "\n",
    "        # Update the existing dictionary in the list\n",
    "        # Find the dict for the current file\n",
    "        entry_to_update = next((item for item in comparison_results_list if item[\"file\"] == relative_path), None)\n",
    "\n",
    "        if entry_to_update:\n",
    "             entry_to_update[\"tuned_model_output\"] = result_str_tuned\n",
    "             if result_str_tuned.startswith(\"Error:\"):\n",
    "                 logging.error(f\"Tuned model error for {relative_path}: {result_str_tuned}\")\n",
    "                 files_error_tuned += 1\n",
    "             else:\n",
    "                 tuned_model_times.append(duration)\n",
    "                 entry_to_update[\"tuned_model_time_sec\"] = duration\n",
    "                 logging.info(f\"Tuned model success for {relative_path} in {duration:.2f}s\")\n",
    "        else:\n",
    "             # This case should ideally not happen if base run added all files\n",
    "             logging.error(f\"Could not find result entry for {relative_path} to update tuned model output.\")\n",
    "             # Optionally create a new entry if needed, though it indicates a logic issue\n",
    "             # comparison_results_list.append({\n",
    "             #     \"file\": relative_path,\n",
    "             #     \"base_model_output\": \"N/A\",\n",
    "             #     \"base_model_time_sec\": None,\n",
    "             #     \"tuned_model_output\": result_str_tuned,\n",
    "             #     \"tuned_model_time_sec\": duration if not result_str_tuned.startswith(\"Error:\") else None,\n",
    "             # })\n",
    "\n",
    "\n",
    "    total_end_time_tuned = time.time()\n",
    "    logging.info(f\"Tuned model run finished. Total time: {total_end_time_tuned - total_start_time_tuned:.2f}s\")\n",
    "    logging.info(f\"Tuned model: {files_processed_tuned - files_error_tuned} successful, {files_error_tuned} errors.\")\n",
    "\n",
    "    logging.info(\"\\nComparison processing finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Save Full Results to JSON ---\n",
    "\n",
    "# Save the detailed comparison results\n",
    "comparison_output_path = os.path.join(output_dir, COMPARISON_OUTPUT_FILENAME)\n",
    "if comparison_results_list: # Only save if there are results\n",
    "    try:\n",
    "        with open(comparison_output_path, 'w', encoding='utf-8') as f:\n",
    "            # Use indent for readability\n",
    "            json.dump(comparison_results_list, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Full comparison results saved to: {comparison_output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving comparison JSON: {e}\")\n",
    "else:\n",
    "    print(\"No comparison results generated to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318beff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Create and Display Pandas DataFrame ---\n",
    "\n",
    "if comparison_results_list:\n",
    "    # Convert the list of dictionaries to a Pandas DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_results_list)\n",
    "\n",
    "    # Optionally save the DataFrame to CSV\n",
    "    comparison_table_path = os.path.join(output_dir, COMPARISON_TABLE_FILENAME)\n",
    "    try:\n",
    "        comparison_df.to_csv(comparison_table_path, index=False)\n",
    "        print(f\"Comparison table saved to: {comparison_table_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving comparison CSV: {e}\")\n",
    "\n",
    "\n",
    "    # Display the first few rows of the DataFrame in the notebook\n",
    "    print(\"\\n--- Comparison Results Table (First 5 Rows) ---\")\n",
    "    # Configure pandas display options for better readability if needed\n",
    "    pd.set_option('display.max_colwidth', 100) # Show more text in columns\n",
    "    pd.set_option('display.max_rows', 10)      # Limit rows displayed initially\n",
    "    display(comparison_df.head())\n",
    "\n",
    "    # Display basic stats for timing columns (ignoring NaNs)\n",
    "    print(\"\\n--- Timing Statistics (seconds per successful file) ---\")\n",
    "    # Calculate stats only on non-error rows where time is not None\n",
    "    timing_stats = comparison_df[['base_model_time_sec', 'tuned_model_time_sec']].dropna().describe()\n",
    "    if not timing_stats.empty:\n",
    "         display(timing_stats)\n",
    "    else:\n",
    "         print(\"No successful timing data available for statistics.\")\n",
    "\n",
    "else:\n",
    "    print(\"No comparison results to display in DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Generate and Display Comparison Graph ---\n",
    "\n",
    "# Calculate average times only from successful runs\n",
    "avg_time_base_ms = None\n",
    "avg_time_tuned_ms = None\n",
    "\n",
    "if base_model_times:\n",
    "    avg_time_base_ms = (sum(base_model_times) / len(base_model_times)) * 1000 # Convert to ms\n",
    "    print(f\"Average Time Base Model (Successful Runs): {avg_time_base_ms:.2f} ms ({len(base_model_times)} files)\")\n",
    "else:\n",
    "    print(\"No successful runs recorded for the base model.\")\n",
    "\n",
    "if tuned_model_times:\n",
    "    avg_time_tuned_ms = (sum(tuned_model_times) / len(tuned_model_times)) * 1000 # Convert to ms\n",
    "    print(f\"Average Time Tuned Model (Successful Runs): {avg_time_tuned_ms:.2f} ms ({len(tuned_model_times)} files)\")\n",
    "else:\n",
    "    print(\"No successful runs recorded for the tuned model.\")\n",
    "\n",
    "\n",
    "# Plotting only if both models had successful runs\n",
    "if avg_time_base_ms is not None and avg_time_tuned_ms is not None:\n",
    "    models = [f'Base\\n({BASE_MODEL_ID})', f'Tuned\\n(ID: ...{TUNED_MODEL_ID[-6:]})'] # Shorten labels\n",
    "    avg_times = [avg_time_base_ms, avg_time_tuned_ms]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6)) # Create figure and axes objects\n",
    "    bars = ax.bar(models, avg_times, color=['skyblue', 'lightgreen'])\n",
    "    ax.set_ylabel('Average Time per File (ms)')\n",
    "    ax.set_title('Model Speed Comparison (Avg. Time for Successful Analyses)')\n",
    "    # Set y-limit dynamically based on the larger average time\n",
    "    ax.set_ylim(0, max(avg_times) * 1.2) # Add 20% padding above the taller bar\n",
    "\n",
    "    # Add text labels to bars\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        # Format the label to have 0 decimal places for milliseconds\n",
    "        ax.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.0f} ms', va='bottom', ha='center', fontsize=9)\n",
    "\n",
    "    # Save the graph\n",
    "    graph_path = os.path.join(output_dir, GRAPH_OUTPUT_FILENAME)\n",
    "    try:\n",
    "        plt.savefig(graph_path)\n",
    "        print(f\"\\nComparison graph saved to: {graph_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving comparison graph: {e}\")\n",
    "\n",
    "    # Display the plot inline in the notebook\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"\\nCannot generate time comparison graph: Insufficient successful runs for one or both models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
